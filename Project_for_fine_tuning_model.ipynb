{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZerocoderForstudents/Project-for-fine-tuning-model/blob/main/Project_for_fine_tuning_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"align-center\"> <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a> <a href=\"https://ollama.com/\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/ollama.png\" height=\"44\"></a>\n"
      ],
      "metadata": {
        "id": "zv8tk44A1pUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"zerocodemethodist@gmail.com\"\n",
        "!git config --global user.name \"ZerocoderForstudents\"\n",
        "!git add .\n",
        "!git commit -m \"Commit message\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6NvkUGn0Kqi",
        "outputId": "7bfbbbd5-c53f-4c66-b012-f51fc71fae14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Latest build\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Supports Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes and other models.\n",
        "* Supports 16-bit LoRA and 4-bit QLoRA. Both options are 2 times faster.\n",
        "* The max_seq_length parameter can be set to any value, as automatic RoPE scaling is used via [kaiokendev's](https://kaiokendev.github.io/til) method\n",
        "\n",
        "* [NEW] Phi-3 Medium / Mini is 2x faster!"
      ],
      "metadata": {
        "id": "030OYCyH4fCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # Choose any value! RoPE scaling support.\n",
        "dtype = None  # None for auto-detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+.\n",
        "load_in_4bit = True  # Use 4-bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# list of available models\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3, 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3, 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3, 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma, 2.2x faster!\n",
        "]  # More models can be found at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # Use token if working with models that require gated access (e.g., meta-llama/Llama-2-7b-hf)\n",
        ")\n"
      ],
      "metadata": {
        "id": "4KuenCh45Vpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding LoRA to update a portion of the weights"
      ],
      "metadata": {
        "id": "xxwNBjBnBNur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # Choose any number > 0! Recommended values: 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,  # Supports any value, but 0 is optimal\n",
        "    bias = \"none\",     # Supports any value, but \"none\" is optimal\n",
        "    # [NEW] ""unsloth"" uses 30% less VRAM, allows for 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\",  # True or ""unsloth"" for very long context\n",
        "    random_state = 3407,\n",
        "    # use_rslora = False,  # Supports rank stabilized LoRA\n",
        "    # loftq_config = None,  # And LoftQ\n",
        ")\n"
      ],
      "metadata": {
        "id": "5My9pIiS68Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a dataset from HF, it is already in the required format and is a datasets class object\n",
        "\n",
        "The [Dataset](https://huggingface.co/datasets/Amod/mental_health_counseling_conversations?row=1) is a collection of questions and answers taken from two platforms for online counseling and therapy. The questions cover a wide range of mental health topics, and the answers are provided by qualified psychologists.\n",
        "\n",
        "The dataset is intended for fine-tuning language models to improve their ability to provide mental health counseling.\n",
        "\n",
        "This can be useful when creating chatbots that provide consultations to people\n"
      ],
      "metadata": {
        "id": "9FUpuw0PBa1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split = \"train\")\n",
        "print(dataset.column_names)"
      ],
      "metadata": {
        "id": "HvOPfPnet76H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also split the dataset into two parts - training and testing samples"
      ],
      "metadata": {
        "id": "mZ47M2kZCdsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)  # 20% of data for the test set\n",
        "\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "Xt_ClEqp1abZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4_dG-m0Cz4"
      },
      "source": [
        "A problem may arise where some datasets contain multiple columns. For Ollama and llama.cpp to work like a ChatGPT chatbot, the dataset must only have two columns—instruction and output."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve this problem, we will do the following:\n",
        "\n",
        "* Combine all columns into one prompt.\n",
        "* Use the *to_sharegpt* function to perform the column merging process!"
      ],
      "metadata": {
        "id": "RH3uLBks8K71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To combine multiple columns into one, use **merged_prompt**.\n",
        "\n",
        "* Enclose all columns in curly braces {}.\n",
        "* Optional text must be enclosed in [[]]. For example, if the ""Pclass"" column is empty, the merge function will not show this text and will skip it. This is useful for datasets with missing values.\n",
        "* You can select all columns or just a few!\n",
        "* Select the prediction column in output_column_name. For our dataset, this is - Response\n",
        "* *Optional*: For fine-tuning to support multiple message exchanges (like in ChatGPT), you need to create a ""fake"" dataset with multiple messages in the dialogue—for this, we use conversation_extension to randomly select several conversations from the dataset and combine them into one conversation"
      ],
      "metadata": {
        "id": "217KJxDA8aJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import to_sharegpt\n",
        "dataset = to_sharegpt(\n",
        "    dataset,\n",
        "    merged_prompt = \"{Context}\",\n",
        "    output_column_name = \"Response\",\n",
        "    # conversation_extension = 1,\n",
        ")"
      ],
      "metadata": {
        "id": "jZxeGSeX0CR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kh90vpD1jYJ"
      },
      "source": [
        "Use **standardize_sharegpt** to bring the dataset to the required format!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)"
      ],
      "metadata": {
        "id": "ZPwDXBvP1g8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThrcKACxTe2"
      },
      "source": [
        "### Setting up the Chat Template (Prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template = \"\"\"Below are some conversations. Write responses that appropriately complete each request.\n",
        "\n",
        "### Question:\n",
        "{INPUT}\n",
        "\n",
        "### Corresponding answer:\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
        ")"
      ],
      "metadata": {
        "id": "JOGaZf1sdLlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "R6kMC_Iy4L0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the model answers questions before fine-tuning"
      ],
      "metadata": {
        "id": "uwhpA2mjDP25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Model acceleration\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Start generation time\n",
        "start_time = time.time()\n",
        "# Generate text\n",
        "output_ids = model.generate(input_ids, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# End generation time\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent on generation\n",
        "generation_time = end_time - start_time\n",
        "print(f\"Generation time: {generation_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "N1tQyH0W8ct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode tokens into string\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "rAQVweYKhv7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode tokens into string\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "cbQEaG0t9Rpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_ids)"
      ],
      "metadata": {
        "id": "UgM0yisA80a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['text'][1])  # Will show the first 10 elements from the 'text' column"
      ],
      "metadata": {
        "id": "Hb-iwup_e2I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Fine-tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "pbiPqHe6D7n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use **SFTTrainer** from the Huggingface TRL library! More details in the documentation: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer).\n",
        "\n",
        "We will run only 60 steps during fine-tuning, but you can set num_train_epochs=1 for full training and disable max_steps=None.\n",
        "\n",
        "TRL's DPOTrainer is also supported!"
      ],
      "metadata": {
        "id": "CxzcpeHoD-0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 2,\n",
        "        max_steps = 60,\n",
        "        # num_train_epochs = 3, # For long fine-tuning\n",
        "        learning_rate = 0.001,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title System Status\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# Disable wandb logging\n",
        "wandb.init(mode='disabled')"
      ],
      "metadata": {
        "id": "_DxZjDIx4xsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting Fine-tuning"
      ],
      "metadata": {
        "id": "pa4CD_8NFPtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show Final Memory Usage and Time Statistics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "### Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the model generates text after fine-tuning"
      ],
      "metadata": {
        "id": "D7ANkY9VFtFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Start generation time\n",
        "start_time = time.time()\n",
        "# Generate text\n",
        "output_ids = model.generate(input_ids, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# End generation time\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent on generation\n",
        "generation_time = end_time - start_time\n",
        "print(f\"Generation time: {generation_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "wLxej7hKpArx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode tokens into string\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "f0-3kuu4pAry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "### Saving and Loading Fine-tuned Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving Only LoRA Weights\n",
        "\n",
        "To save the final model as LoRA adapters, use Huggingface's push_to_hub for saving to HF or save_pretrained for local saving.\n",
        "\n",
        "[NOTE] This only saves the LoRA adapters, not the entire model.\n",
        "To save the model in 16bit or GGUF format, scroll down!"
      ],
      "metadata": {
        "id": "7nBzK5n2GbYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "\n",
        "Now, if you want to load the LoRA adapters we just saved for inference, replace `False` with `True`:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # your fine-tuned model\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "pass\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFzC441vCtq"
      },
      "source": [
        "<a name=\"Ollama\"></a>\n",
        "#### Ollama Support\n",
        "\n",
        "Unsloth allows fine-tuning models, creating a Modelfile, and exporting them to Ollama! This significantly simplifies the fine-tuning process and ensures a smooth workflow from Unsloth to Ollama!\n",
        "\n",
        "Let's install Ollama first"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "NUxcyP_UfeLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's save the model in GGUF format\n",
        "\n",
        "Use the save_pretrained_gguf method for local model saving and push_to_hub_gguf for uploading to the Hugging Face Hub.\n",
        "By default, the model is saved in q8_0 format.\n",
        "\n",
        "Some supported quantization methods, more can be found on the [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options):\n",
        "\n",
        "q8_0 — Fast conversion. Requires many resources, but generally acceptable.\n",
        "\n",
        "q4_k_m — Recommended method. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, the rest is Q4_K.\n",
        "\n",
        "q5_k_m — Recommended method. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, the rest is Q5_K."
      ],
      "metadata": {
        "id": "uXYwqFE0HesT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to 8bit Q8_0\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Go to https://huggingface.co/settings/tokens for the HF token\n",
        "# Change hf to your name!\n",
        "# Create a model named 'model' in your profile to save the model into it\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to various GGUF formats\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your name!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Go to https://huggingface.co/settings/tokens for the HF token\n",
        "    )"
      ],
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uploading the Model to HF"
      ],
      "metadata": {
        "id": "y0OR0Eas_4xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True: model.push_to_hub_gguf(\"katya1836/model\", tokenizer, token = \"\")"
      ],
      "metadata": {
        "id": "guMzFT-Cssfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and running Ollama using Python"
      ],
      "metadata": {
        "id": "t_U6PkDFJHnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "ZYcXi_7e_BKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull hf.co/katya1836/model"
      ],
      "metadata": {
        "id": "eW9ZYob71R-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "### Question:\n",
        "I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\n",
        "\n",
        "### Corresponding answer:\n",
        "\"\"\"\n",
        "\n",
        "response = requests.post(\n",
        "    'http://localhost:11434/api/generate',\n",
        "    json={\n",
        "        'model': \"hf.co/katya1836/model\",\n",
        "        \"options\": { \"temperature\": 0, \"num_predict\": 150},\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        ")\n",
        "\n",
        "# Decode the response content\n",
        "result = response.content.decode('utf-8')\n",
        "result_json = json.loads(result)\n",
        "# Extract the text from the 'response' key\n",
        "generated_text = result_json.get('response')\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "xXhPV-Um_Eqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uploading the Model to Ollama"
      ],
      "metadata": {
        "id": "70nNbJioLb7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run inference using this model via Ollama!"
      ],
      "metadata": {
        "id": "_jVrVcb5LEl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "CdXh5UMNgLN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ollama requires a model file (Modelfile) that specifies the prompt format for the model. Let's output the automatically generated model file from Unsloth"
      ],
      "metadata": {
        "id": "9Xm_EAe6J_cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ],
      "metadata": {
        "id": "h82vfNigRhiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create an Ollama model named unsloth_model using the automatically generated Modelfile!"
      ],
      "metadata": {
        "id": "ZTQYHLv1KJzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ],
      "metadata": {
        "id": "wNijsEumiywt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run unsloth_model"
      ],
      "metadata": {
        "id": "tYaXy24XgLN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "### Question:\n",
        "I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\n",
        "\n",
        "### Corresponding answer:\n",
        "\"\"\"\n",
        "\n",
        "response = requests.post(\n",
        "    'http://localhost:11434/api/generate',\n",
        "    json={\n",
        "        'model': \"unsloth_model\",\n",
        "        \"options\": { \"temperature\": 0, \"num_predict\": 150},\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        ")\n",
        "\n",
        "# Decode the response content\n",
        "result = response.content.decode('utf-8')\n",
        "result_json = json.loads(result)\n",
        "# Extract the text from the 'response' key\n",
        "generated_text = result_json.get('response')\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "eLa82OyigLN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Command Line Simulation"
      ],
      "metadata": {
        "id": "sfF3tBBZKUPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import JSON\n",
        "from google.colab import output\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "def shell(command):\n",
        "  if command.startswith('cd'):\n",
        "    path = command.strip().split(maxsplit=1)[1]\n",
        "    os.chdir(path)\n",
        "    return JSON([''])\n",
        "  return JSON([getoutput(command)])\n",
        "output.register_callback('shell', shell)"
      ],
      "metadata": {
        "id": "q0hyOUsRPs7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Shell\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(async function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
        "              let out = res.data['application/json'][0]\n",
        "              this.echo(new String(out))\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to Colab Shell',\n",
        "      name: 'colab_demo',\n",
        "      height: 250,\n",
        "      prompt: 'colab > '\n",
        "  });"
      ],
      "metadata": {
        "id": "TEMABh7QLLDK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}